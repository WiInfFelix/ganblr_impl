{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_FLAGS=\"--xla_gpu_cuda_data_dir=/usr/lib/cuda/\"\n"
     ]
    }
   ],
   "source": [
    "%env XLA_FLAGS=\"--xla_gpu_cuda_data_dir=/usr/lib/cuda/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 17:45:39.602340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-01 17:45:39.602374: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-01 17:45:39.602406: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-01 17:45:39.609568: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-01 17:45:40.216590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "import pandas as pd\n",
    "import tracemalloc\n",
    "# import numpy as np\n",
    "from ganblr.models import GANBLR\n",
    "from data_utils import (\n",
    "    transfrom_dataframe_discrete,\n",
    "    preprocess_superstore,\n",
    "    preprocess_credit_risk,\n",
    "    preprocess_mushroom\n",
    ")\n",
    "from logger_utils import CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from metric_utils import get_trtr_metrics, get_sdv_metrics\n",
    "from datetime import datetime\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = [150]\n",
    "K = [1]\n",
    "\n",
    "overall_logfile = Path(f\"./new_logs/log_{datetime.now().strftime('%Y%m%d-%H%M%S')}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/lib/cuda/' \n",
    "\n",
    "timestamp_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "with open(overall_logfile, \"w\") as f:\n",
    "    # write with ; as delimiter\n",
    "    f.write(\"Event;Model;Epochs;K;Dataset;Test;Metric;Value\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_name, X, y, df_enc, encoders, X_train, X_test, y_train, y_test, epochs, K, timestamp_id, overall_logfile):\n",
    "    for epoch in epochs:\n",
    "        for k in K:\n",
    "            for i in range(1, 4):\n",
    "                ganblr = GANBLR()\n",
    "                ganblr.fit(X, y, epochs=epoch, k=k)\n",
    "\n",
    "                # sample as many rows as the original dataset\n",
    "                synth_data = pd.DataFrame(\n",
    "                    ganblr.sample(X.shape[0]),\n",
    "                    columns=df_enc.columns,\n",
    "                )\n",
    "\n",
    "                # decode the categorical columns\n",
    "                synth_data_clear = synth_data.copy()\n",
    "                for col in df_enc.columns:\n",
    "                    synth_data_clear[col] = encoders[col].inverse_transform(\n",
    "                        synth_data[[col]].astype(int)\n",
    "                    )\n",
    "\n",
    "                synth_data_clear.to_csv(f\"./synth_data/{timestamp_id}_ganblr_synth_data_{dataset_name}_{epoch}_{k}_{i}.csv\")\n",
    "\n",
    "                # get metrics\n",
    "                get_trtr_metrics(\n",
    "                    X_train,\n",
    "                    X_test,\n",
    "                    y_train,\n",
    "                    y_test,\n",
    "                    synth_data,\n",
    "                    dataset_name,\n",
    "                    \"GANBLR\",\n",
    "                    overall_logfile,\n",
    "                    epoch,\n",
    "                    k,\n",
    "                )\n",
    "\n",
    "                get_sdv_metrics(\n",
    "                    real_data=df_enc,\n",
    "                    synth_data=synth_data,\n",
    "                    dataset_name=dataset_name,\n",
    "                    model=\"GANBLR\",\n",
    "                    overall_logfile=overall_logfile,\n",
    "                    epochs=epoch,\n",
    "                    k=k,\n",
    "                    timestamp=timestamp_id,\n",
    "                    i=i\n",
    "                )\n",
    "\n",
    "                del ganblr\n",
    "                gc.collect()\n",
    "        \n",
    "        for i in range(1, 4):\n",
    "            metadata = SingleTableMetadata()\n",
    "            metadata.detect_from_dataframe(data=df_enc)\n",
    "            ctgan = CTGANSynthesizer(metadata, epochs=epoch)\n",
    "            ctgan.fit(df_enc)\n",
    "\n",
    "            synth_data_ctgan = pd.DataFrame(\n",
    "                ctgan.sample(X.shape[0]),\n",
    "                columns=df_enc.columns,\n",
    "            )\n",
    "\n",
    "            synth_data_ctgan_clear = synth_data_ctgan.copy()\n",
    "            for col in df_enc.columns:\n",
    "                synth_data_ctgan_clear[col] = encoders[col].inverse_transform(\n",
    "                    synth_data_ctgan[[col]].astype(int)\n",
    "                )\n",
    "\n",
    "            synth_data_ctgan_clear.to_csv(f\"./synth_data/{timestamp_id}_ctgan_synth_data_{dataset_name}_{epoch}_{k}_{i}.csv\")\n",
    "\n",
    "            get_trtr_metrics(\n",
    "                X_train,\n",
    "                X_test,\n",
    "                y_train,\n",
    "                y_test,\n",
    "                synth_data_ctgan,\n",
    "                dataset_name,\n",
    "                \"CTGAN\",\n",
    "                overall_logfile,\n",
    "                epoch,\n",
    "                0,\n",
    "            )\n",
    "\n",
    "            get_sdv_metrics(\n",
    "                real_data=df_enc,\n",
    "                synth_data=synth_data_ctgan,\n",
    "                dataset_name=dataset_name,\n",
    "                model=\"CTGAN\",\n",
    "                overall_logfile=overall_logfile,\n",
    "                epochs=epoch,\n",
    "                k=0,\n",
    "                timestamp=timestamp_id,\n",
    "                i=i\n",
    "            )\n",
    "\n",
    "            del ctgan\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/felixs/Documents/ganblr_impl/venv/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SUPERSTORE_PATH = Path(\"datasets/SampleSuperstore.csv\")\n",
    "CREDIT_RISK_PATH = Path(\"datasets/credit_risk_dataset.csv\")\n",
    "MUSHROOMS_PATH = Path(\"datasets/mushrooms.csv\")\n",
    "\n",
    "SUPERSTORE_DF = pd.read_csv(SUPERSTORE_PATH)\n",
    "CREDIT_RISK_DF = pd.read_csv(CREDIT_RISK_PATH)\n",
    "MUSHROOMS_DF = pd.read_csv(MUSHROOMS_PATH)\n",
    "\n",
    "SUPERSTORE_DF = preprocess_superstore(SUPERSTORE_DF)\n",
    "CREDIT_RISK_DF = preprocess_credit_risk(CREDIT_RISK_DF)\n",
    "MUSHROOMS_DF = preprocess_mushroom(MUSHROOMS_DF)\n",
    "\n",
    "SUPERSTORE_DF_ENC, SUPERSTORE_ENCODERS = transfrom_dataframe_discrete(SUPERSTORE_DF)\n",
    "CREDIT_RISK_DF_ENC, CREDIT_RISK_ENCODERS = transfrom_dataframe_discrete(CREDIT_RISK_DF)\n",
    "MUSHROOMS_DF_ENC, MUSHROOMS_ENCODERS = transfrom_dataframe_discrete(MUSHROOMS_DF)\n",
    "\n",
    "\n",
    "# cast all columns to categorical\n",
    "# SUPERSTORE_DF_ENC = SUPERSTORE_DF_ENC.astype(\"category\")\n",
    "# CREDIT_RISK_DF_ENC = CREDIT_RISK_DF_ENC.astype(\"category\")\n",
    "# MUSHROOMS_DF_ENC = MUSHROOMS_DF_ENC.astype(\"category\")\n",
    "\n",
    "X_super = SUPERSTORE_DF_ENC.drop(\"Profit\", axis=1)\n",
    "y_super = SUPERSTORE_DF_ENC[\"Profit\"]\n",
    "\n",
    "X_super_train, X_super_test, y_super_train, y_super_test = train_test_split(\n",
    "    X_super, y_super, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "X_credit = CREDIT_RISK_DF_ENC.drop(\"loan_status\", axis=1)\n",
    "y_credit = CREDIT_RISK_DF_ENC[\"loan_status\"]\n",
    "\n",
    "X_credit_train, X_credit_test, y_credit_train, y_credit_test = train_test_split(\n",
    "    X_credit, y_credit, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_mushrooms = MUSHROOMS_DF_ENC.drop(\"class\", axis=1)\n",
    "y_mushrooms = MUSHROOMS_DF_ENC[\"class\"]\n",
    "\n",
    "X_mushrooms_train, X_mushrooms_test, y_mushrooms_train, y_mushrooms_test = train_test_split(\n",
    "    X_mushrooms, y_mushrooms, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    \"superstore\",\n",
    "    X_super,\n",
    "    y_super,\n",
    "    SUPERSTORE_DF_ENC,\n",
    "    SUPERSTORE_ENCODERS,\n",
    "    X_super_train,\n",
    "    X_super_test,\n",
    "    y_super_train,\n",
    "    y_super_test,\n",
    "    EPOCHS,\n",
    "    K,\n",
    "    timestamp_id,\n",
    "    overall_logfile\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    \"credit_risk\",\n",
    "    X_credit,\n",
    "    y_credit,\n",
    "    CREDIT_RISK_DF_ENC,\n",
    "    CREDIT_RISK_ENCODERS,\n",
    "    X_credit_train,\n",
    "    X_credit_test,\n",
    "    y_credit_train,\n",
    "    y_credit_test,\n",
    "    EPOCHS,\n",
    "    K,\n",
    "    timestamp_id,\n",
    "    overall_logfile\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    \"mushrooms\",\n",
    "    X_mushrooms,\n",
    "    y_mushrooms,\n",
    "    MUSHROOMS_DF_ENC,\n",
    "    MUSHROOMS_ENCODERS,\n",
    "    X_mushrooms_train,\n",
    "    X_mushrooms_test,\n",
    "    y_mushrooms_train,\n",
    "    y_mushrooms_test,\n",
    "    EPOCHS,\n",
    "    K,\n",
    "    timestamp_id,\n",
    "    overall_logfile\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9287.19921875\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "print(process.memory_info().rss / 1024 ** 2)  # in megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%memit` not found.\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
